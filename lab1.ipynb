{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> This introductory lab guides the attendees with the following activities: </h3>\n",
    "\n",
    "1. Setting up the AI Project in the AI Foundry\n",
    "2. Deploying an LLM and embedding models\n",
    "3. Establish connectivity from VS Code into the AI Project\n",
    "4. Perform a simple Chat completion call"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> 1.1 Creating AI Project in AI Foundry </h4>\n",
    "\n",
    "1. Login to AI Foundry at https://ai.azure.com\n",
    "1. Click on Create Project\n",
    "1. Click on Create Hub.\n",
    "1. Click on \"customize\" and customize the resources names as you like. **Please make sure to create the AI Search in the customization screen. You will need the AI Search in the later labs**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<h4> 1.2 Deploy models </h4>\n",
    "\n",
    "1. Under My assets section, click on the Models + endpoints\n",
    "1. Click on Deploy model and Deploy base model\n",
    "1. Select gpt-4o-mini and deploy it\n",
    "1. Similarly deploy an embedding model text-embedding-ada-002 that you will need for the next lab."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> 1.3 Create .env file wiht environment variables </h4>\n",
    "\n",
    "1. create the .env file in the VS code explorer where this Jupyter file is located\n",
    "1. Get the connection string, model names from the AI Project and add them to the .env file like below.\n",
    "\n",
    "AIPROJECT_CONNECTION_STRING=\"eastus.api.azureml.ms;********************;resourcegroup;project\" <br>\n",
    "EMBEDDINGS_MODEL=\"text-embedding-ada-002\" <br>\n",
    "CHAT_MODEL=\"gpt-4o-mini\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>1.4 Proceed with the Jupyter notebook below </h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import python packages\n",
    "from azure.ai.projects import AIProjectClient\n",
    "from azure.identity import DefaultAzureCredential\n",
    "from dotenv import load_dotenv\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eastus.api.azureml.ms;4c61dc39-311c-43a4-bd17-4b18ec045427;rg-agent;project-demo-v2zb\n",
      "gpt-4o-mini\n"
     ]
    }
   ],
   "source": [
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "project_connection_string = os.getenv(\"AIPROJECT_CONNECTION_STRING\")\n",
    "model = os.getenv(\"CHAT_MODEL\")\n",
    "\n",
    "print(project_connection_string)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# connect to the AI project \n",
    "project = AIProjectClient.from_connection_string(\n",
    "    conn_str=project_connection_string, credential=DefaultAzureCredential()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sure! Here it goes:\n",
      "\n",
      "Why did the teddy bear say \"no\" to dessert?\n",
      "\n",
      "Because it was already stuffed! \n"
     ]
    }
   ],
   "source": [
    "# chat with the LLM model\n",
    "chat = project.inference.get_chat_completions_client()\n",
    "response = chat.complete(\n",
    "    model=model,\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"You are an AI assistant that tells jokes for toddlers.\",\n",
    "        },\n",
    "        {\"role\": \"user\", \"content\": \"Hey, can you tell a joke about teddy bear?\"},\n",
    "    ],\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
